{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0c10750f-ac16-4aad-94e0-ce3ea4b7f0cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ee7059ad-09e1-4a81-9b9a-14abc285f03d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords, brown\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from transformers import BertTokenizer\n",
    "import nltk\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4b11ab3c-bcc3-4504-ad05-d48e6ef7bf00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minhd\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\minhd\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "# Load pre-trained ResNet50V2 model\n",
    "resnet_model = models.resnet50(pretrained=True)\n",
    "resnet_model = torch.nn.Sequential(*list(resnet_model.children())[:-1])  # Remove the last layer\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text, tokenizer, max_length=128):\n",
    "    tokens = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        add_special_tokens=True,\n",
    "        return_tensors='pt'  # Use return_tensors='pt' to get PyTorch tensors\n",
    "    )\n",
    "    return tokens['input_ids'][0]  # Return PyTorch tensor without batch dimension\n",
    "\n",
    "from PIL import Image, ImageSequence\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def preprocess_image(image_path, target_size=(256, 256)):\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # If the image is a GIF, extract the first frame\n",
    "    if image.format == 'GIF':\n",
    "        frames = [frame.copy() for frame in ImageSequence.Iterator(image)]\n",
    "        image = frames[0]\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image = transform(image)\n",
    "    return image  # Add batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2f09bb67-cf5d-4f76-9940-ba032661cba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased'):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        \n",
    "        resnet_model = models.resnet50(pretrained=True)\n",
    "        self.resnet = nn.Sequential(\n",
    "            *list(resnet_model.children())[:-2],  # Up to the last conv layer\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),         # Ensure output is (batch_size, 2048, 1, 1)\n",
    "            nn.Flatten()                          # Flatten to (batch_size, 2048)\n",
    "        )\n",
    "        \n",
    "        bert_hidden_size = self.bert.config.hidden_size\n",
    "        resnet_feature_size = 2048  # Fixed feature size for ResNet50\n",
    "        \n",
    "        self.fc1 = nn.Linear(bert_hidden_size + resnet_feature_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, text_input, image_input):\n",
    "        text_outputs = self.bert(text_input)\n",
    "        text_cls_token = text_outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "\n",
    "        image_features = self.resnet(image_input)\n",
    "        \n",
    "        combined_features = torch.cat((text_cls_token, image_features), dim=1)\n",
    "        combined_features = self.dropout(combined_features)\n",
    "        combined_features = torch.relu(self.fc1(combined_features))\n",
    "        combined_features = self.dropout(combined_features)\n",
    "        output = torch.sigmoid(self.fc2(combined_features))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# Function to predict using the combined model\n",
    "def predict(texts, image_paths, tokenizer, model):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess inputs\n",
    "    text_input_ids = torch.stack([preprocess_text(text, tokenizer) for text in texts])\n",
    "    image_input_tensors = torch.stack([preprocess_image(image_path) for image_path in image_paths])\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    text_input_ids = text_input_ids.to(device, dtype=torch.int64)\n",
    "    image_input_tensors = image_input_tensors.to(device, dtype=torch.float)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(text_input_ids, image_input_tensors)\n",
    "    \n",
    "    # Convert output to binary predictions\n",
    "    binary_predictions = (outputs >= 0.9).int().squeeze().tolist()\n",
    "\n",
    "    return binary_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d2a25cc2-d884-4aa1-bb3a-0189da3814b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, image_paths, labels, tokenizer, max_length=128, target_size=(256, 256)):\n",
    "        self.texts = texts\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.target_size = target_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        text_input = preprocess_text(text, self.tokenizer, self.max_length)\n",
    "        image_input = preprocess_image(image_path, self.target_size)\n",
    "        \n",
    "        return text_input, image_input, label\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1b41dd7f-4ec1-4d58-ba07-88559ec78e6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_path = []\n",
    "folder_path = \"C:/Users/minhd/FPTU lab/DPL302m/Kaggle/2024-sum-dpl-302-m/devset_images/devset_images\"\n",
    "for filename in os.listdir(folder_path):\n",
    "    image_path.append(folder_path + \"/\" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "772999df-db8d-4361-a38a-b4557e1b9fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>user_tags</th>\n",
       "      <th>title</th>\n",
       "      <th>license_name</th>\n",
       "      <th>user_nsid</th>\n",
       "      <th>image_extension_original</th>\n",
       "      <th>longitude</th>\n",
       "      <th>image_id</th>\n",
       "      <th>license_url</th>\n",
       "      <th>date_uploaded</th>\n",
       "      <th>date_taken</th>\n",
       "      <th>latitude</th>\n",
       "      <th>image_url</th>\n",
       "      <th>user_nickname</th>\n",
       "      <th>capture_device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>[2009 road trip, obrero road trip]</td>\n",
       "      <td>Biltmore Estate</td>\n",
       "      <td>Attribution-NonCommercial-NoDerivs License</td>\n",
       "      <td>95156977@N00</td>\n",
       "      <td>jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3519864665</td>\n",
       "      <td>http://creativecommons.org/licenses/by-nc-nd/2.0/</td>\n",
       "      <td>1242004112</td>\n",
       "      <td>2009-05-10 08:27:33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.flickr.com/photos/95156977@N00/3519...</td>\n",
       "      <td>5 Flip-Flops (Earl)</td>\n",
       "      <td>Canon EOS DIGITAL REBEL XT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>[daulatabad, daulatabad fort, ellora, road trip]</td>\n",
       "      <td>Chand Minar</td>\n",
       "      <td>Attribution-ShareAlike License</td>\n",
       "      <td>24574470@N00</td>\n",
       "      <td>jpg</td>\n",
       "      <td>75.200386</td>\n",
       "      <td>4896119055</td>\n",
       "      <td>http://creativecommons.org/licenses/by-sa/2.0/</td>\n",
       "      <td>1281931224</td>\n",
       "      <td>2010-08-14 13:35:10.0</td>\n",
       "      <td>19.939383</td>\n",
       "      <td>http://www.flickr.com/photos/24574470@N00/4896...</td>\n",
       "      <td>sankarshan</td>\n",
       "      <td>NIKON CORPORATION NIKON D90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After the flood, the boarded up stores bear up...</td>\n",
       "      <td>[cedarrapids, createsouthroadtrip2009, disaste...</td>\n",
       "      <td>Uplifting Graffiti</td>\n",
       "      <td>Attribution License</td>\n",
       "      <td>73451168@N00</td>\n",
       "      <td>jpg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3468473862</td>\n",
       "      <td>http://creativecommons.org/licenses/by/2.0/</td>\n",
       "      <td>1240493762</td>\n",
       "      <td>2009-04-21 18:07:56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.flickr.com/photos/73451168@N00/3468...</td>\n",
       "      <td>J Wynia</td>\n",
       "      <td>Panasonic DMC-TZ5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>[cork, enchente, flood, ireland, irlanda]</td>\n",
       "      <td>DSCF6487</td>\n",
       "      <td>Attribution-NonCommercial-NoDerivs License</td>\n",
       "      <td>12947023@N00</td>\n",
       "      <td>jpg</td>\n",
       "      <td>-8.621177</td>\n",
       "      <td>4120853942</td>\n",
       "      <td>http://creativecommons.org/licenses/by-nc-nd/2.0/</td>\n",
       "      <td>1258754762</td>\n",
       "      <td>2009-11-20 15:16:40.0</td>\n",
       "      <td>51.889603</td>\n",
       "      <td>http://www.flickr.com/photos/12947023@N00/4120...</td>\n",
       "      <td>guileite</td>\n",
       "      <td>FUJIFILM FinePix S6000fd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>[athens georgia, brown, current, flood, mud, r...</td>\n",
       "      <td>Oconoe river - flooded</td>\n",
       "      <td>Attribution License</td>\n",
       "      <td>60704492@N00</td>\n",
       "      <td>jpg</td>\n",
       "      <td>-83.368265</td>\n",
       "      <td>4436083254</td>\n",
       "      <td>http://creativecommons.org/licenses/by/2.0/</td>\n",
       "      <td>1268676971</td>\n",
       "      <td>2010-03-13 15:14:04.0</td>\n",
       "      <td>33.949149</td>\n",
       "      <td>http://www.flickr.com/photos/60704492@N00/4436...</td>\n",
       "      <td>The_Gut</td>\n",
       "      <td>Canon PowerShot SX10 IS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description  \\\n",
       "0                                               None   \n",
       "1                                               None   \n",
       "2  After the flood, the boarded up stores bear up...   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                           user_tags                   title  \\\n",
       "0                 [2009 road trip, obrero road trip]         Biltmore Estate   \n",
       "1   [daulatabad, daulatabad fort, ellora, road trip]             Chand Minar   \n",
       "2  [cedarrapids, createsouthroadtrip2009, disaste...      Uplifting Graffiti   \n",
       "3          [cork, enchente, flood, ireland, irlanda]                DSCF6487   \n",
       "4  [athens georgia, brown, current, flood, mud, r...  Oconoe river - flooded   \n",
       "\n",
       "                                 license_name     user_nsid  \\\n",
       "0  Attribution-NonCommercial-NoDerivs License  95156977@N00   \n",
       "1              Attribution-ShareAlike License  24574470@N00   \n",
       "2                         Attribution License  73451168@N00   \n",
       "3  Attribution-NonCommercial-NoDerivs License  12947023@N00   \n",
       "4                         Attribution License  60704492@N00   \n",
       "\n",
       "  image_extension_original  longitude    image_id  \\\n",
       "0                      jpg        NaN  3519864665   \n",
       "1                      jpg  75.200386  4896119055   \n",
       "2                      jpg        NaN  3468473862   \n",
       "3                      jpg  -8.621177  4120853942   \n",
       "4                      jpg -83.368265  4436083254   \n",
       "\n",
       "                                         license_url date_uploaded  \\\n",
       "0  http://creativecommons.org/licenses/by-nc-nd/2.0/    1242004112   \n",
       "1     http://creativecommons.org/licenses/by-sa/2.0/    1281931224   \n",
       "2        http://creativecommons.org/licenses/by/2.0/    1240493762   \n",
       "3  http://creativecommons.org/licenses/by-nc-nd/2.0/    1258754762   \n",
       "4        http://creativecommons.org/licenses/by/2.0/    1268676971   \n",
       "\n",
       "              date_taken   latitude  \\\n",
       "0  2009-05-10 08:27:33.0        NaN   \n",
       "1  2010-08-14 13:35:10.0  19.939383   \n",
       "2  2009-04-21 18:07:56.0        NaN   \n",
       "3  2009-11-20 15:16:40.0  51.889603   \n",
       "4  2010-03-13 15:14:04.0  33.949149   \n",
       "\n",
       "                                           image_url        user_nickname  \\\n",
       "0  http://www.flickr.com/photos/95156977@N00/3519...  5 Flip-Flops (Earl)   \n",
       "1  http://www.flickr.com/photos/24574470@N00/4896...           sankarshan   \n",
       "2  http://www.flickr.com/photos/73451168@N00/3468...              J Wynia   \n",
       "3  http://www.flickr.com/photos/12947023@N00/4120...             guileite   \n",
       "4  http://www.flickr.com/photos/60704492@N00/4436...              The_Gut   \n",
       "\n",
       "                capture_device  \n",
       "0   Canon EOS DIGITAL REBEL XT  \n",
       "1  NIKON CORPORATION NIKON D90  \n",
       "2            Panasonic DMC-TZ5  \n",
       "3     FUJIFILM FinePix S6000fd  \n",
       "4      Canon PowerShot SX10 IS  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Đọc trực tiếp file JSON thành DataFrame\n",
    "df = pd.read_json('devset_images_metadata.json')\n",
    "\n",
    "# Nếu cần chuyển đổi từ cột chứa danh sách các đối tượng thành các cột DataFrame\n",
    "df = pd.json_normalize(df['images'])\n",
    "\n",
    "# Hiển thị DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "33810e66-5516-4dfc-80f3-83a6f4dedf9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_label = pd.read_csv('devset_images_gt.csv')\n",
    "train_label.rename(columns = {'id': 'image_id', 'label': 'train_y'}, inplace = True)\n",
    "data = pd.concat([df, train_label], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6c4a08fb-77d5-404d-be20-7ab29b982d04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_user_tags(tags):\n",
    "    if isinstance(tags, list):\n",
    "        return ' '.join(tags)\n",
    "    elif pd.isnull(tags):\n",
    "        return '[NULL]'\n",
    "    else:\n",
    "        return tags\n",
    "\n",
    "data['user_tags'] = data['user_tags'].apply(preprocess_user_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a6aeac71-6751-4043-9634-6f7d6c3f2ca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data['text'] = data[['description', 'user_tags','title']].apply(lambda x: ' | '.join(x.dropna()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "00587aca-0166-4595-aaab-333ec1879c9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = data['text'].tolist()\n",
    "labels = data['train_y'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "190f3034-d6fd-4bc0-8121-3b15598cd5ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from autocorrect import Speller\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8d89fe61-253d-44d8-b82f-59e21baf1cba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\minhd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\minhd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8e65e58f-bfdb-4f77-906c-b7c8cc9f0daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = CustomDataset(texts, image_path, labels, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size= 32, shuffle= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "fc91348e-9835-41cd-b0b7-fe230fcb92b3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minhd\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\minhd\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[195], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(text_inputs, image_inputs)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[0;32m     23\u001b[0m predicted \u001b[38;5;241m=\u001b[39m (outputs \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mint()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[142], line 26\u001b[0m, in \u001b[0;36mCombinedModel.forward\u001b[1;34m(self, text_input, image_input)\u001b[0m\n\u001b[0;32m     23\u001b[0m text_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(text_input)\n\u001b[0;32m     24\u001b[0m text_cls_token \u001b[38;5;241m=\u001b[39m text_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# CLS token\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnet(image_input)\n\u001b[0;32m     28\u001b[0m combined_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((text_cls_token, image_features), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     29\u001b[0m combined_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(combined_features)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torchvision\\models\\resnet.py:148\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    146\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m    147\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m--> 148\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m    150\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n\u001b[0;32m    151\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:103\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1498\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(relu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m-> 1498\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1500\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = CombinedModel()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop with accuracy calculation\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for text_inputs, image_inputs, labels in dataloader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(text_inputs, image_inputs)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predicted = (outputs >= 0.5).int()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6765942f-1fc1-4f96-8d7d-517b1cbc1c15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "test_df['text'] = test_df[['description', 'user_tags','title']].apply(lambda x: ' | '.join(x.dropna()), axis=1)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_user_tags)\n",
    "test_texts = test_df['text'].to_list()\n",
    "for i in range(len(test_texts)):\n",
    "    test_texts[i] = clean_text(test_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e603a4-bf27-4f40-b6e3-f6975dd166a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_image_path = []\n",
    "test_folder_path = \"C:/Users/minhd/FPTU lab/DPL302m/Kaggle/2024-sum-dpl-302-m/testset_images/testset_images\"\n",
    "for filename in os.listdir(test_folder_path):\n",
    "    test_image_path.append(test_folder_path + \"/\" + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b61fc0-2b6e-475a-b5e8-0b8deb7b7323",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = predict(test_texts, test_image_path, tokenizer, model)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "924beca2-a145-4f91-bbf0-78c8c0abee5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3483809003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3712805295</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>379845620</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7343264988</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3843337492</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  label\n",
       "0  3483809003      0\n",
       "1  3712805295      0\n",
       "2   379845620      0\n",
       "3  7343264988      1\n",
       "4  3843337492      1"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit = pd.DataFrame({'id': test_df['image_id'], 'label': predictions})\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4ac7d76b-4880-4309-992d-99dde0dc7b73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_csv_path = 'Allin.csv'\n",
    "submit.to_csv(results_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48da7c9b-1207-4db3-a553-8d5c92dd2bce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
